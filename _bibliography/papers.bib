---
---

@string{aps = {American Physical Society,}}


@inproceedings{lumos,
  title={Lumos: Learning Agents with Unified Data, Modular Design, and Open-Source LLMs},
  author={Da Yin and
          Faeze Brahman and
          Abhilasha Ravichander and
          Khyathi Chandu and
          Kai{-}Wei Chang and
          Yejin {Choi} and
          Bill Yuchen Lin},
  booktitle={arXiv},
  month=Nov,
  year={2023},
  url={https://arxiv.org/pdf/2311.05657},
  preview={lumos.png},
  arxiv={2311.05657},
  code={https://github.com/allenai/lumos},
  website={https://allenai.github.io/lumos/},
  model={https://huggingface.co/models?sort=trending&search=ai2lumos},
  abstract={We introduce Lumos, a novel framework for training language agents that employs a unified data format and a modular architecture based on open-source large language models (LLMs). Lumos consists of three distinct modules: planning, grounding, and execution. The planning module breaks down a task into a series of high-level, tool-agnostic subgoals, which are then made specific by the grounding module through a set of low-level actions. These actions are subsequently executed by the execution module, utilizing a range of off-the-shelf tools and APIs. In order to train these modules effectively, high-quality annotations of subgoals and actions were collected and are made available for fine-tuning open-source LLMs for various tasks such as complex question answering, web tasks, and math problems. Leveraging this unified data and modular design, Lumos not only achieves comparable or superior performance to current, state-of-the-art agents, but also exhibits several key advantages: (1) Lumos surpasses GPT-4/3.5-based agents in complex question answering and web tasks, while equalling the performance of significantly larger LLM agents on math tasks; (2) Lumos outperforms open-source agents created through conventional training methods and those using chain-of-thoughts training; and (3) Lumos is capable of effectively generalizing to unseen interactive tasks, outperforming larger LLM-based agents and even exceeding performance of specialized agents.}
}


@inproceedings{mmysymkd,
  title={Localized Symbolic Knowledge Distillation for Visual Commonsense Models},
  author={Jae Sung Park and Jack Hessel and Khyathi Chandu and Paul Pu Liang and Ximing Lu and Peter West and Youngjae Yu and Qiuyuan Huang and Jianfeng {Gao} and Ali {Farhadi} and Yejin {Choi}},
  booktitle={Neurips},
  month=Nov,
  year={2023},
  url={https://openreview.net/pdf?id=V5eG47pyVl},
  preview={mmsymkd.png},
  code={https://github.com/jamespark3922/lskd},
  abstract={Instruction following vision-language (VL) models offer a flexible interface that supports a broad range of multimodal tasks in a zero-shot fashion. However, interfaces that operate on full images do not directly enable the user to “point to" and access specific regions within images. This capability is important not only to support reference-grounded VL benchmarks, but also, for practical applications that require precise within-image reasoning. We build Localized Visual Commonsense model which allows users to specify (multiple) regions- as-input. We train our model by sampling localized commonsense knowledge from a large language model (LLM): specifically, we prompt a LLM to collect commonsense knowledge given a global literal image description and a local literal region description automatically generated by a set of VL models. This pipeline is scalable and fully automatic, as no aligned or human-authored image and text pairs are required. With a separately trained critic model that selects high quality examples, we find that training on the localized commonsense corpus expanded solely from images can successfully distill existing VL models to support a reference-as-input interface. Empirical results and human evaluations in zero-shot settings demonstrate that our distillation method results in more precise VL models of reasoning compared to a baseline of passing a generated referring expression.}
}

@inproceedings{paradox,
  title={The Generative AI Paradox: "What It Can Create, It May Not Understand"},
  author={Peter West and Ximing Lu and Nouha Dziri and Faeze Brahman and Linjie Li and Jena D. Hwang and Liwei Jiang and Jillian Fisher and Abhilasha Ravichander and Khyathi Chandu and Benjamin Newman and Pang Wei Koh and Allyson Ettinger and Yejin {Choi}},
  booktitle={arXiv},
  month=Oct,
  year={2023},
  url={https://arxiv.org/abs/2311.00059},
  preview={paradox.png},
  arxiv={2311.00059},
  abstract={The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models' generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.}
}

@inproceedings{lowrecorp,
  title={LOWRECORP: the Low-Resource NLG Corpus Building Challenge},
  author={Khyathi Raghavi Chandu and David M Howcroft and Dimitra Gkatzia and Yi-Ling Chung and Yufang Hou and Chris Chinenye Emezue and Pawan Rajpoot and Tosin Adewumi},
  booktitle={INLG},
  month=Sept,
  year={2023},
  arxiv={https://arxiv.org/pdf/2311.05657},
  preview={lowrecorp.png},
  arxiv={2311.05657},
  website={https://},
  abstract={Most languages in the world do not have sufficient data available to develop neural-network-based natural language generation (NLG) systems. To alleviate this resource scarcity, we propose a novel challenge for the NLG community: low-resource language corpus development (LOWRECORP). We present an innovative framework to collect a single dataset with dual tasks to maximize the efficiency of data collection efforts and respect language consultant time. Specifically, we focus on a text-chat-based interface for two generation tasks–conversational response generation grounded in a source document and/or image and dialogue summarization (from the former task). The goal of this shared task is to collectively develop grounded datasets for local and low-resourced languages. To enable data collection, we make available web-based software that can be used to collect these grounded conversations and summaries. Submissions will be assessed for the size, complexity, and diversity of the corpora to ensure quality control of the datasets as well as any enhancements to the interface or novel approaches to grounding conversations.}
}


@inproceedings{tulu,
  title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources},
  author={Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A Smith and Iz Beltagy and Hannaneh Hajishirzi},
  booktitle={Neurips},
  month=June,
  year={2023},
  url={https://arxiv.org/pdf/2306.04751},
  preview={tulu.png},
  arxiv={2306.04751},
  code={https://github.com/allenai/open-instruct},
  model={https://huggingface.co/models?sort=trending&search=ai2lumos},
  abstract={In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce Tülu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources. Our experiments show that different instruction-tuning datasets can uncover or enhance specific skills, while no single dataset (or combination) provides the best performance across all evaluations. Interestingly, we find that model and human preference-based evaluations fail to reflect differences in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of systemic evaluation performed in this work. Our evaluations show that the best model in any given evaluation reaches on average 87% of ChatGPT performance, and 73% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap. We release our instruction-tuned models, including a fully finetuned 65B Tülu, along with our code, data, and evaluation framework at this https URL to facilitate future research.}
}

@inproceedings{ipa,
  title={Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning},
  author={Ximing Lu and Faeze Brahman and Peter West and Jaehun Jang and Khyathi Chandu and Abhilasha Ravichander and Lianhui Qin and Prithviraj Ammanabrolu and Liwei Jiang and Sahana Ramnath and Nouha Dziri and Jillian Fisher and Bill Yuchen Lin and Skyler Hallinan and Xiang Ren and Sean Welleck and Yejin {Choi}},
  booktitle={EMNLP},
  month=Dec,
  year={2023},
  url={https://arxiv.org/abs/2305.15065.pdf},
  preview={ipa.png},
  arxiv={2305.15065.pdf},
  abstract={While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.}
}


@inproceedings{needle,
  title={A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization},
  author={Lining Zhang and Simon Mille and Yufang Hou and Daniel Deutsch and Elizabeth Clark and Yixin Liu and Saad Mahamood and Sebastian {Gehrmann} and Miruna Clinciu and Khyathi Raghavi Chandu and João Sedoc},
  booktitle={ACL},
  month=July,
  year={2023},
  url={https://aclanthology.org/2023.acl-long.835.pdf},
  preview={needle.png},
  arxiv={2023.acl-long.835.pdf},
  abstract={To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources. Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness. This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.}
}



@inproceedings{cdst,
  title={Continual Dialogue State Tracking via Example-Guided Question Answering},
  author={Hyundong Cho and Andrea Madotto and Zhaojiang Lin and Khyathi Raghavi Chandu and Satwik Kottur and Jing Xu and Jonathan May and Chinnadhurai Sankar},
  booktitle={EMNLP},
  month=May,
  year={2023},
  url={https://arxiv.org/pdf/2305.13721},
  preview={cdst.png},
  arxiv={2305.13721},
  abstract={Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new
services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial
component of dialogue systems that estimates the user’s goal as a conversation proceeds, is a
simple natural language understanding task, we propose reformulating it as a bundle of granular example-guided question answering tasks
to minimize the task shift between services and thus benefit continual learning. Our approach
alleviates service-specific memorization and teaches a model to contextualize the given question and example to extract the necessary information from the conversation. We find that a
model with just 60M parameters can achieve a significant boost by learning to learn from
in-context examples retrieved by a retriever trained to identify turns with similar dialogue
state changes. Combining our method with dialogue-level memory replay, our approach attains state of the art performance on DST continual learning metrics without relying on any
complex regularization or parameter expansion methods.}
}

